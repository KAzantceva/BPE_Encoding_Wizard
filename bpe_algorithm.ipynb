{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the final project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is devoted to the question-answering task. You are going to work with the **BoolQ** dataset from SuperGLUE .\n",
    "\n",
    "BoolQ is a question answering dataset for yes/no. \n",
    "\n",
    "Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The dataset release consists of three `.jsonl` files (`train, val, test`), where each line is a JSON dictionary with the following format:\n",
    "\n",
    "    Example:\n",
    "    \n",
    "    {\n",
    "      \"question\": \"is france the same timezone as the uk\",\n",
    "      \"passage\": \"At the Liberation of France in the summer of 1944, Metropolitan France kept GMT+2 as it was the time then used by the Allies (British Double Summer Time). In the winter of 1944--1945, Metropolitan France switched to GMT+1, same as in the United Kingdom, and switched again to GMT+2 in April 1945 like its British ally. In September 1945, Metropolitan France returned to GMT+1 (pre-war summer time), which the British had already done in July 1945. Metropolitan France was officially scheduled to return to GMT+0 on November 18, 1945 (the British returned to GMT+0 in on October 7, 1945), but the French government canceled the decision on November 5, 1945, and GMT+1 has since then remained the official time of Metropolitan France.\"\n",
    "      \"label\": false,\n",
    "      \"idx\": 123,\n",
    "    }\n",
    "\n",
    "For this project you will need only `train` and `val` parts.\n",
    "\n",
    "Dataset is available here: https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\n",
    "\n",
    "Detailed task description is available here: https://arxiv.org/abs/1905.10044\n",
    "\n",
    "## Bonus for those, who want to work with Russian\n",
    "\n",
    "For those who want to work with Russian data there is an option to use **DaNetQA** dataset (instead of BoolQ) from Russsian SuperGLUE. The dataset is organised similarly to BoolQ.\n",
    "\n",
    "You may download data from here: https://russiansuperglue.com/tasks/download/DaNetQA\n",
    "\n",
    "Detailed task description is available here: https://arxiv.org/abs/2010.02605\n",
    "\n",
    "\n",
    "**Note:** note that you should take only one dataset (either BoolQ or DanetQA). There are no bonus points for solving both tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Task Description\n",
    "\n",
    "In the task you should perform all the model training on `train.jsonl` data and evaluate your models on `val.jsonl`. Note that you do not need `test.jsonl`.\n",
    "\n",
    "Use **accuracy metric** for scoring.\n",
    "\n",
    "The solution of either BoolQ or DaNetQA should include:\n",
    "\n",
    "\n",
    "### [10%] 1. Data analysis \n",
    "\n",
    "\n",
    "* 1.1  (10%) Download the data and calculate basic statistics (example number, class distribution, mean sentence length, number of unique words, etc.) and make visualizations, if necessary. \n",
    "\n",
    "\n",
    "### [40%] 2. Pretrained embeddings as features for classifier\n",
    "* 2.1 (5%) Take pre-trained word2vec or fastText embeddings and vectorize your data using them.\n",
    "\n",
    "**Note:** you may use any pre-trained embeddings available on the Internet. For example, for English you may take GoogleNews vectors, which are available here: https://www.kaggle.com/leadbest/googlenewsvectorsnegative300 . For the Russian language you may choose one of the models from RusVectores (https://rusvectores.org/ru/models/).\n",
    "\n",
    "* 2.2  (15%) Use pretrained embeddings as features to a classifier. Train SVM, Linear Regression or any other classification model. Describe and analyze your results (use accuracy metric and data from `val.jsonl` for evaluation of your results).\n",
    "\n",
    "* 2.3 (5%) Instead of word2vec or fastText embeddings get BERT embeddings from the train dataset and vectorize your data. \n",
    "\n",
    "**Note:** you may use any of the BERT-like models from hugging face (https://huggingface.co/) library. For example, you may use `bert-base-multilingual-cased` (https://huggingface.co/bert-base-multilingual-cased).\n",
    "\n",
    "* 2.4  (15%) Similarly to 2.1, train SVM, Linear Regression or any other classification model using BERT embeddings as features for a classifier. Describe and analyze your results (use accuracy metric and data from `val.jsonl` for evaluation of your results).\n",
    "\n",
    "\n",
    "### [40%] 3. Fine-tune BERT\n",
    "\n",
    "* 3.1 (10%) Split the data from `train.jsonl` into train and dev (dev_size = 10%) . Tokenize and format the data (do not forget about the [SEP] token).\n",
    "\n",
    "* 3.2   (10%) Initialize the model, optimizer and learning rate scheduler. Explain your choice of the parameters.\n",
    "\n",
    "* 3.3   (10%) Write a training loop and fine-tune BERT. Do to forget about evaluation on dev set created in 3.1. \n",
    "\n",
    "**Note:** you may use one of the tutorials for fine-tuning BERT, but you have to provide a link to it. For example, you may use this one: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "* 3.4   (10%)  Make predictions for the test data (from `val.jsonl`),  and analyze the results (use accuracy metric for scoring).\n",
    "\n",
    "\n",
    "\n",
    "### [10%] 4. Summary & results analysis\n",
    "\n",
    "\n",
    "* 4.1    (5%) Compare the results of all the tested models and try to interpret them.\n",
    "\n",
    "* 4.2    (5%) Propose, how you may improve the model score (write at least three ideas).\n",
    "\n",
    "\n",
    "### General Requirements\n",
    "\n",
    "* You should provide a solution in the form of the Jupyter Notebook with code cells and markdown cells (with text)\n",
    "* Make sure that the instructor can run all the cells to reproduce your results\n",
    "* Clearly answer each question, perform the required actions from the task. Explain your decisions if you choose to use some techniques\n",
    "* Provide links to the tutorials you use for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 9427\n",
      "Class distribution:\n",
      "True     5874\n",
      "False    3553\n",
      "Name: label, dtype: int64\n",
      "Mean sentence length: 97.44733213111276\n",
      "Number of unique words: 88465\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading data\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Counting the number of examples and classes\n",
    "num_examples = len(df)\n",
    "class_distribution = df['label'].value_counts()\n",
    "\n",
    "# Counting the average length of sentences\n",
    "df['sentence_length'] = df['passage'].apply(lambda x: len(x.split()))\n",
    "mean_sentence_length = np.mean(df['sentence_length'])\n",
    "\n",
    "# Counting the number of unique words\n",
    "unique_words = set(' '.join(df['passage']).split())\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_distribution.index, class_distribution.values)\n",
    "plt.xticks([0, 1], ['False', 'True'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Statistical output\n",
    "print(f'Number of examples: {num_examples}')\n",
    "print(f'Class distribution:\\n{class_distribution}')\n",
    "print(f'Mean sentence length: {mean_sentence_length}')\n",
    "print(f'Number of unique words: {num_unique_words}')\n",
    "\n",
    "# Separation of data into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6150583244962884\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Loading pre-trained vectors \n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Function for text vectorization using Word2Vec\n",
    "def text_to_vector(text, model, vector_size):\n",
    "    words = text.split()\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            vector += model[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "\n",
    "# Convert text data to vectors\n",
    "vector_size = 300  \n",
    "train_vectors = [text_to_vector(text, word2vec_model, vector_size) for text in train_df['passage']]\n",
    "val_vectors = [text_to_vector(text, word2vec_model, vector_size) for text in val_df['passage']]\n",
    "\n",
    "# Training a classification model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train an SVM model to account for the unequal distribution of classes\n",
    "# Set class weights to account for the imbalance\n",
    "class_weights = {\n",
    "    0: len(train_df) / (2 * (train_df['label'] == 0).sum()),  \n",
    "    1: len(train_df) / (2 * (train_df['label'] == 1).sum())  \n",
    "}\n",
    "svm_classifier = SVC(class_weight=class_weights)\n",
    "\n",
    "svm_classifier.fit(train_vectors, train_df['label'])\n",
    "\n",
    "# Prediction on validation set\n",
    "val_predictions = svm_classifier.predict(val_vectors)\n",
    "\n",
    "# Model estimation using an accuracy metric\n",
    "accuracy = accuracy_score(val_df['label'], val_predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model based on pre-trained Word2Vec embeddings showed acceptable performance on validation data.\n",
    "The accuracy could be improved by more careful selection of SVM parameters or by using other classifiers.\n",
    "Despite this, the Word2Vec-based model did not achieve the accuracy we obtained using BERT-like models.\n",
    "Overall, using pre-trained Word2Vec embeddings as features provides a model for text classification, but for higher accuracy and improved performance, BERT-like models may be a better choice for this text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 531/531 [1:39:19<00:00, 11.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 343.5084535777569, Validation Accuracy: 0.6882290562036055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 531/531 [1:38:12<00:00, 11.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 302.3935408592224, Validation Accuracy: 0.6924708377518558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 531/531 [1:23:59<00:00,  9.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 235.90201152861118, Validation Accuracy: 0.7051961823966065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initializing tokenizer and BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  \n",
    "\n",
    "# Data Tokenization\n",
    "max_length = 128 \n",
    "train_texts = train_df['question'] + \" [SEP] \" + train_df['passage']\n",
    "val_texts = val_df['question'] + \" [SEP] \" + val_df['passage']\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert bool to int\n",
    "train_labels = train_df['label'].astype(int)\n",
    "val_labels = val_df['label'].astype(int)\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels.values, dtype=torch.int64))\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels.values, dtype=torch.int64))\n",
    "\n",
    "# Separation of data into training and validation sets\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initializing the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  \n",
    "num_epochs = 3  \n",
    "\n",
    "# Learning Cycle\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "   # Estimation of the model on the validation set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "# Saving the model\n",
    "model.save_pretrained('bert_question_answering_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 205/205 [12:48<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6602446483180429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initializing tokenizer and BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained('bert_question_answering_model')  \n",
    "\n",
    "# Loading and tokenization of validation data (val.jsonl)\n",
    "with open('val.jsonl', 'r', encoding='utf-8') as file:\n",
    "    val_data = [json.loads(line) for line in file]\n",
    "\n",
    "val_texts = [item['passage'] for item in val_data]\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "\n",
    "# Convert bool to int\n",
    "val_labels = [item['label'] for item in val_data]\n",
    "\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels, dtype=torch.int64))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Estimation of the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc='Validation'):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "val_accuracy = correct / total\n",
    "print(f'Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing and analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results and interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the results of two models that were developed as part of the assignment: a model using pre-trained Word2Vec or FastText embeddings and a model using pre-trained BERT embeddings. We will evaluate the results using an accuracy metric as well as analyze the results:\n",
    "\n",
    "The model using Word2Vec/FastText showed an accuracy on the validation dataset of 0.6882. This means that the model correctly classified 68.82% of the examples.\n",
    "\n",
    "The model using BERT showed an accuracy on the validation dataset equal to 0.7052. This means that the model tuned using BERT correctly classified 70.52% of the examples.\n",
    "\n",
    "Interpretation of results: the model using BERT showed better performance compared to the model using Word2Vec/FastText. This indicates that BERT-like models are able to extract more informative features and improve text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestion for Improvements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve model estimation, we can consider the following ideas:\n",
    "\n",
    "Fine-tuning BERT: We could further fine-tune BERT with a more thorough search for optimal hyperparameters and longer training. This could help the model perform better.\n",
    "\n",
    "Using other BERT models: We could try other BERT models such as bert-base-multilingual-cased to determine which one is better suited for the task.\n",
    "\n",
    "Data Augmentation: Data augmentation techniques such as synonym substitution, generating similar sentences, etc. can be applied to improve the model performance.\n",
    "\n",
    "Feature Engineering: We can consider creating additional text related features that can help the model in classification.\n",
    "\n",
    "Ensemble Exploration: We can try to create ensembles of multiple models to improve the results.\n",
    "\n",
    "These measures can help to improve the performance of the model and its ability to classify text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
